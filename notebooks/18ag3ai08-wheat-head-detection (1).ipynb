{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Name: Aviral Jain | \nRoll No: 18AG3AI08 | \nBTP Guide: Prof. Rajendra Machavaram\n\n# Application of Convolutional Neural Networks for Wheat Head Detection","metadata":{}},{"cell_type":"markdown","source":"Firstly we set up the required environment experiment and install necessary libraries and dependencies. ","metadata":{}},{"cell_type":"code","source":"!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install pycocotools \n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2 \nimport torchvision\nfrom torchvision import datasets,transforms\nfrom tqdm import tqdm\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nimport torch.nn.functional as F\nimport ast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-22T18:03:03.05426Z","iopub.execute_input":"2021-10-22T18:03:03.054945Z","iopub.status.idle":"2021-10-22T18:03:03.061809Z","shell.execute_reply.started":"2021-10-22T18:03:03.054882Z","shell.execute_reply":"2021-10-22T18:03:03.061109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configure the hyperparameters here\nLR = 1e-4\nSPLIT = 0.2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 4\nEPOCHS = 2\nDATAPATH = '../input/global-wheat-detection'","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:24.645972Z","iopub.execute_input":"2021-10-22T17:51:24.646259Z","iopub.status.idle":"2021-10-22T17:51:24.685803Z","shell.execute_reply.started":"2021-10-22T17:51:24.64622Z","shell.execute_reply":"2021-10-22T17:51:24.685128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls '../input/global-wheat-detection'","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:24.687093Z","iopub.execute_input":"2021-10-22T17:51:24.687502Z","iopub.status.idle":"2021-10-22T17:51:25.378377Z","shell.execute_reply.started":"2021-10-22T17:51:24.687465Z","shell.execute_reply":"2021-10-22T17:51:25.377494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(DATAPATH + '/train.csv')\ndf.bbox = df.bbox.apply(ast.literal_eval)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:25.380086Z","iopub.execute_input":"2021-10-22T17:51:25.380396Z","iopub.status.idle":"2021-10-22T17:51:27.541133Z","shell.execute_reply.started":"2021-10-22T17:51:25.380357Z","shell.execute_reply":"2021-10-22T17:51:27.540313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.groupby(\"image_id\")[\"bbox\"].apply(list).reset_index(name=\"bboxes\")","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.543476Z","iopub.execute_input":"2021-10-22T17:51:27.543765Z","iopub.status.idle":"2021-10-22T17:51:27.776414Z","shell.execute_reply.started":"2021-10-22T17:51:27.543725Z","shell.execute_reply":"2021-10-22T17:51:27.775477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(dataFrame,split):\n    len_tot = len(dataFrame)\n    val_len = int(split*len_tot)\n    train_len = len_tot-val_len\n    train_data,val_data = dataFrame.iloc[:train_len][:],dataFrame.iloc[train_len:][:]\n    return train_data,val_data","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.778736Z","iopub.execute_input":"2021-10-22T17:51:27.779307Z","iopub.status.idle":"2021-10-22T17:51:27.78532Z","shell.execute_reply.started":"2021-10-22T17:51:27.779264Z","shell.execute_reply":"2021-10-22T17:51:27.78445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df,val_data_df = train_test_split(df,SPLIT)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.786826Z","iopub.execute_input":"2021-10-22T17:51:27.787114Z","iopub.status.idle":"2021-10-22T17:51:27.796355Z","shell.execute_reply.started":"2021-10-22T17:51:27.787079Z","shell.execute_reply":"2021-10-22T17:51:27.795206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.797691Z","iopub.execute_input":"2021-10-22T17:51:27.79904Z","iopub.status.idle":"2021-10-22T17:51:27.935278Z","shell.execute_reply.started":"2021-10-22T17:51:27.799002Z","shell.execute_reply":"2021-10-22T17:51:27.934601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WheatDataset(Dataset):     #Create a dataset class which reads the images and bounding boxes and passes them into the dataloader in the correct format\n    def __init__(self,data,root_dir,transform=None,train=True):\n        self.data = data\n        self.root_dir = root_dir\n        self.image_names = self.data.image_id.values\n        self.bboxes = self.data.bboxes.values\n        self.transform = transform\n        self.isTrain = train\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self,index):\n#         print(self.image_names)\n#         print(self.bboxes)\n        img_path = os.path.join(self.root_dir,self.image_names[index]+\".jpg\")\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        bboxes = torch.tensor(self.bboxes[index],dtype=torch.float64)\n#         print(bboxes)\n        \"\"\"\n            As per the docs of torchvision\n            we need bboxes in format (xmin,ymin,xmax,ymax)\n            Currently we have them in format (xmin,ymin,width,height)\n        \"\"\"\n        bboxes[:,2] = bboxes[:,0]+bboxes[:,2]\n        bboxes[:,3] = bboxes[:,1]+bboxes[:,3]\n#         print(image.size,type(image))\n        \"\"\"\n            we need to return image and a target dictionary\n            target:\n                boxes,labels,image_id,area,iscrowd\n        \"\"\"\n        area = (bboxes[:,3]-bboxes[:,1])*(bboxes[:,2]-bboxes[:,0])\n        area = torch.as_tensor(area,dtype=torch.float32)\n        \n        # there is only one class\n        labels = torch.ones((len(bboxes),),dtype=torch.int64)\n        \n        # suppose all instances are not crowded\n        iscrowd = torch.zeros((len(bboxes),),dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = bboxes\n        target['labels']= labels\n        target['image_id'] = torch.tensor([index])\n        target[\"area\"] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transform is not None:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transform(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            \n        return image,target","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.936521Z","iopub.execute_input":"2021-10-22T17:51:27.936923Z","iopub.status.idle":"2021-10-22T17:51:27.951433Z","shell.execute_reply.started":"2021-10-22T17:51:27.936875Z","shell.execute_reply":"2021-10-22T17:51:27.950602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply necessary transformations\n\ntrain_transform = A.Compose([\n    A.Flip(0.5),\n    ToTensorV2(p=1.0)\n],bbox_params = {'format':\"pascal_voc\",'label_fields': ['labels']})\n\nval_transform = A.Compose([\n      ToTensorV2(p=1.0)\n],bbox_params = {'format':\"pascal_voc\",\"label_fields\":['labels']})","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.952673Z","iopub.execute_input":"2021-10-22T17:51:27.953097Z","iopub.status.idle":"2021-10-22T17:51:27.964746Z","shell.execute_reply.started":"2021-10-22T17:51:27.95306Z","shell.execute_reply":"2021-10-22T17:51:27.964073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.965873Z","iopub.execute_input":"2021-10-22T17:51:27.966247Z","iopub.status.idle":"2021-10-22T17:51:27.974484Z","shell.execute_reply.started":"2021-10-22T17:51:27.966211Z","shell.execute_reply":"2021-10-22T17:51:27.973713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = WheatDataset(train_data_df,DATAPATH+\"/train\",transform=train_transform)\nvalid_data = WheatDataset(val_data_df,DATAPATH+\"/train\",transform=val_transform)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.975395Z","iopub.execute_input":"2021-10-22T17:51:27.977367Z","iopub.status.idle":"2021-10-22T17:51:27.983978Z","shell.execute_reply.started":"2021-10-22T17:51:27.977331Z","shell.execute_reply":"2021-10-22T17:51:27.983304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image,target = train_data.__getitem__(0)\nprint(image.shape)\n\nplt.imshow(image.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:27.986751Z","iopub.execute_input":"2021-10-22T17:51:27.986975Z","iopub.status.idle":"2021-10-22T17:51:28.525475Z","shell.execute_reply.started":"2021-10-22T17:51:27.986953Z","shell.execute_reply":"2021-10-22T17:51:28.524794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the Faster R-CNN Model","metadata":{}},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n#We use the pretrained Faster-RCNN so that our training is faster\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:28.527871Z","iopub.execute_input":"2021-10-22T17:51:28.52817Z","iopub.status.idle":"2021-10-22T17:51:29.234052Z","shell.execute_reply.started":"2021-10-22T17:51:28.528127Z","shell.execute_reply":"2021-10-22T17:51:29.233109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total=0.0\n        self.iterations = 0.0\n    def send(self,value):\n        self.current_total+=value\n        self.iterations+=1\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0*self.current_total/self.iterations\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:03:17.082755Z","iopub.execute_input":"2021-10-22T18:03:17.083233Z","iopub.status.idle":"2021-10-22T18:03:17.091767Z","shell.execute_reply.started":"2021-10-22T18:03:17.083197Z","shell.execute_reply":"2021-10-22T18:03:17.090894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create torch based dataloader so that training is accelerated by GPU","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn)\nval_dataloader = DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn)\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:29.246382Z","iopub.execute_input":"2021-10-22T17:51:29.247351Z","iopub.status.idle":"2021-10-22T17:51:29.255954Z","shell.execute_reply.started":"2021-10-22T17:51:29.247313Z","shell.execute_reply":"2021-10-22T17:51:29.255203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = []\nval_loss = []\nmodel = model.to(DEVICE)\nparams =[p for p in model.parameters() if p.requires_grad]\n#We compile the model using Adaptive Momentum Optimizer\noptimizer = optim.Adam(params,lr=LR)\nloss_hist = Averager()\nitr = 1\nlr_scheduler=None","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:51:29.257205Z","iopub.execute_input":"2021-10-22T17:51:29.257939Z","iopub.status.idle":"2021-10-22T17:51:32.104102Z","shell.execute_reply.started":"2021-10-22T17:51:29.257889Z","shell.execute_reply":"2021-10-22T17:51:32.103316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Start training","metadata":{}},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\n\nloss_hist = Averager()\nitr = 1\nmodel = model.float()\nfor epoch in range(EPOCHS):\n    loss_hist.reset()\n    \n    for images, targets in train_dataloader:\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        print(losses.dtype)\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")  \n    evaluate(model, val_dataloader, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:56:35.622525Z","iopub.execute_input":"2021-10-22T17:56:35.623092Z","iopub.status.idle":"2021-10-22T17:56:35.907884Z","shell.execute_reply.started":"2021-10-22T17:56:35.623052Z","shell.execute_reply":"2021-10-22T17:56:35.90683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the training weights","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the saved weights","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"../input/fasterrcnn-weights/fasterrcnn_resnet50_fpn_best.pth\"))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:34.527943Z","iopub.execute_input":"2021-10-22T18:13:34.528638Z","iopub.status.idle":"2021-10-22T18:13:34.663624Z","shell.execute_reply.started":"2021-10-22T18:13:34.528596Z","shell.execute_reply":"2021-10-22T18:13:34.662831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's test the model and see the results","metadata":{}},{"cell_type":"code","source":"images, targets = next(iter(val_dataloader))\nimages = list(img.to(DEVICE) for img in images)\nprint(images[0].shape)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:34.665402Z","iopub.execute_input":"2021-10-22T18:13:34.665823Z","iopub.status.idle":"2021-10-22T18:13:34.818832Z","shell.execute_reply.started":"2021-10-22T18:13:34.665785Z","shell.execute_reply":"2021-10-22T18:13:34.818134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ncpu_device = torch.device(\"cpu\")\n# print(images[0].shape)\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:34.820171Z","iopub.execute_input":"2021-10-22T18:13:34.820589Z","iopub.status.idle":"2021-10-22T18:13:35.024296Z","shell.execute_reply.started":"2021-10-22T18:13:34.82055Z","shell.execute_reply":"2021-10-22T18:13:35.023554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:35.025565Z","iopub.execute_input":"2021-10-22T18:13:35.025919Z","iopub.status.idle":"2021-10-22T18:13:35.517078Z","shell.execute_reply.started":"2021-10-22T18:13:35.025873Z","shell.execute_reply":"2021-10-22T18:13:35.516198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_paths = pd.read_csv(os.path.join(DATAPATH,'sample_submission.csv'))\ntest_img_paths = test_imgs_paths.image_id.values\ntest_dir = DATAPATH+\"/test\"","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:35.519065Z","iopub.execute_input":"2021-10-22T18:13:35.519562Z","iopub.status.idle":"2021-10-22T18:13:35.535815Z","shell.execute_reply.started":"2021-10-22T18:13:35.519524Z","shell.execute_reply":"2021-10-22T18:13:35.535114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=[]\nimage_ids=[]\nwith torch.no_grad():\n    for path in test_img_paths:\n        img_path = os.path.join(test_dir,path+\".jpg\")\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        image = np.transpose(image,(2,0,1))\n    #     print(image.shape)\n        image = torch.tensor(image,dtype=torch.float)\n        image = image.unsqueeze(0)\n    #     print(image.shape)\n        image = image.to(DEVICE)\n        outputs = model(image)\n\n        predict=[]\n        outputs = outputs[0]\n        for i in range(len(outputs['boxes'])):\n            temp = np.array([str(outputs['scores'][i].item()),str(outputs['boxes'][i][0].item()),str(outputs['boxes'][i][1].item()),str(outputs['boxes'][i][2].item()-outputs['boxes'][i][0].item()),str(outputs['boxes'][i][3].item()-outputs['boxes'][i][1].item())])\n            predict.append(temp)\n        predict = np.array(predict).flatten()\n        predict = ' '.join(predict.flatten())\n        image_ids.append(path)\n        predictions.append(predict)\n    print(\"------------Generating Submission File---------\")\n    df = pd.DataFrame({\"image_id\":image_ids,\"PredictionString\":predictions})\n    df.to_csv('./submission.csv.gz',index=False,compression='gzip')","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:13:35.536802Z","iopub.execute_input":"2021-10-22T18:13:35.537028Z","iopub.status.idle":"2021-10-22T18:13:36.536155Z","shell.execute_reply.started":"2021-10-22T18:13:35.537004Z","shell.execute_reply":"2021-10-22T18:13:36.535357Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use Intersection Over Union (IOU) metric as a benchmark.","metadata":{}},{"cell_type":"code","source":"def bb_intersection_over_union(boxA, boxB):\n\t# determine the (x, y)-coordinates of the intersection rectangle\n\txA = max(boxA[0], boxB[0])\n\tyA = max(boxA[1], boxB[1])\n\txB = min(boxA[2], boxB[2])\n\tyB = min(boxA[3], boxB[3])\n\t# compute the area of intersection rectangle\n\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\t# compute the area of both the prediction and ground-truth\n\t# rectangles\n\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\t# compute the intersection over union by taking the intersection\n\t# area and dividing it by the sum of prediction + ground-truth\n\t# areas - the interesection area\n\tiou = interArea / float(boxAArea + boxBArea - interArea)\n\t# return the intersection over union value\n\treturn interArea, float(boxAArea + boxBArea - interArea)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:54:07.058425Z","iopub.execute_input":"2021-10-22T18:54:07.059036Z","iopub.status.idle":"2021-10-22T18:54:07.065743Z","shell.execute_reply.started":"2021-10-22T18:54:07.058995Z","shell.execute_reply":"2021-10-22T18:54:07.065013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we try the model on the validation set, and see the result. We also calculate the IOU of each prediction.","metadata":{}},{"cell_type":"code","source":"images, targets = next(iter(val_dataloader))\nimages = list(img.to(DEVICE) for img in images)\n#print(images[0].shape)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\nboxes = targets[1]\n#sample = images[1].permute(1,2,0).cpu().numpy()\n\nmodel.eval()\ncpu_device = torch.device(\"cpu\")\n# print(images[0].shape)\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n#outputs = outputs\n\nval_iou = []\n\nfor i in range(len(outputs)):\n    b1 = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    b2 = outputs[i]['boxes'].detach().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    \n    scores = outputs[i]['scores'].detach().numpy()\n   # print(scores)\n    #b2 = b2[scores >= 0.5].astype(np.int32)\n    #print(b1.shape)\n    #print(b1)\n    \n    b2 = [x for _ , x in sorted(zip(scores,b2), key=lambda x: x[0], reverse = True)]\n    b2 = b2[:(b1.shape[0])]\n    b_good = []\n    #print(len(b2))\n    numerator = 0\n    denominator = 0\n    \n    for bb2 in b2:\n        for bb1 in b1:\n            #print(bb1, bb2)\n            o = bb_intersection_over_union(bb1, bb2)\n           # print(o)\n            num = o[0]\n            dem = o[1]\n            if num>0 and num/dem>0.5:\n                numerator += num\n                denominator +=dem\n                b_good.append(bb2)\n    iou = numerator/denominator\n    print(iou)\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in b1:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n    for box in b_good:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 0, 220), 3)\n    ax.set_axis_off()\n    ax.imshow(sample)\n    \n    \n    \nprint(val_iou)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T19:20:28.155847Z","iopub.execute_input":"2021-10-22T19:20:28.156711Z","iopub.status.idle":"2021-10-22T19:20:30.613686Z","shell.execute_reply.started":"2021-10-22T19:20:28.156675Z","shell.execute_reply":"2021-10-22T19:20:30.612821Z"},"trusted":true},"execution_count":null,"outputs":[]}]}